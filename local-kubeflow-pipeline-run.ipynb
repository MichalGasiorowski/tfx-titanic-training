{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous training with TFX and Google Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "1.  Use the TFX CLI to build a TFX pipeline.\n",
    "2.  Deploy a TFX pipeline version without tuning to a hosted AI Platform Pipelines instance.\n",
    "3.  Create and monitor a TFX pipeline run using the TFX CLI.\n",
    "4.  Deploy a new TFX pipeline version with tuning enabled to a hosted AI Platform Pipelines instance.\n",
    "5.  Create and monitor another TFX pipeline run directly in the KFP UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you use utilize the following tools and services to deploy and run a TFX pipeline on Google Cloud that automates the development and deployment of a TensorFlow 2.3 WideDeep Classifer to predict forest cover from cartographic data:\n",
    "\n",
    "* The [**TFX CLI**](https://www.tensorflow.org/tfx/guide/cli) utility to build and deploy a TFX pipeline.\n",
    "* A hosted [**AI Platform Pipeline instance (Kubeflow Pipelines)**](https://www.tensorflow.org/tfx/guide/kubeflow) for TFX pipeline orchestration.\n",
    "* [**Dataflow**](https://cloud.google.com/dataflow) jobs for scalable, distributed data processing for TFX components.\n",
    "* A [**AI Platform Training**](https://cloud.google.com/ai-platform/) job for model training and flock management for parallel tuning trials. \n",
    "* [**AI Platform Prediction**](https://cloud.google.com/ai-platform/) as a model server destination for blessed pipeline model versions.\n",
    "* [**CloudTuner**](https://www.tensorflow.org/tfx/guide/tuner#tuning_on_google_cloud_platform_gcp) and [**AI Platform Vizier**](https://cloud.google.com/ai-platform/optimizer/docs/overview) for advanced model hyperparameter tuning using the Vizier algorithm.\n",
    "\n",
    "You will then create and monitor pipeline runs using the TFX CLI as well as the KFP UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update lab environment PATH to include TFX CLI and skaffold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/michal/.local/bin:/home/michal/venv/ML-tfx-0.30.0/bin:/home/michal/google-cloud-sdk/bin:/home/michal/anaconda3/condabin:/home/michal/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Set `PATH` to include the directory containing TFX CLI and skaffold.\n",
    "PATH=%env PATH\n",
    "HOME=%env HOME\n",
    "\n",
    "%env PATH={HOME}/.local/bin:{PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate lab package version installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFX version: 0.30.0\n",
      "KFP version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import tfx; print('TFX version: {}'.format(tfx.__version__))\"\n",
    "!python -c \"import kfp; print('KFP version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) If running the above command results in different package versions or you receive an import error, upgrade to the correct versions by running the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --user tfx==0.30.0\n",
    "%pip install --upgrade --user kfp==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you may need to restart the kernel to pick up the correct package versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup local path to data, train, test folders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "try: notebook_path\n",
    "except NameError: \n",
    "    notebook_path=os.getcwd() # to only run it once\n",
    "\n",
    "local_data_dirpath = os.path.join(notebook_path, 'data')\n",
    "local_code_dirpath = os.path.join(notebook_path, 'pipeline')\n",
    "\n",
    "local_train_dirpath = os.path.join(local_data_dirpath, \"train\")\n",
    "local_train_filepath = os.path.join(local_train_dirpath, \"train.csv\")\n",
    "local_test_dirpath = os.path.join(local_data_dirpath, \"test\")\n",
    "local_test_filepath = os.path.join(local_test_dirpath, \"test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!rm kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate creation of AI Platform Pipelines cluster\n",
    "\n",
    "Navigate to [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page in the Google Cloud Console.\n",
    "\n",
    "Note you may have already deployed an AI Pipelines instance during the Setup for the lab series. If so, you can proceed using that instance. If not:\n",
    "\n",
    "**1.  Create or select an existing Kubernetes cluster (GKE) and deploy AI Platform**. Make sure to select `\"Allow access to the following Cloud APIs https://www.googleapis.com/auth/cloud-platform\"` to allow for programmatic access to your pipeline by the Kubeflow SDK for the rest of the lab. Also, provide an `App instance name` such as \"tfx\" or \"mlops\". \n",
    "\n",
    "Validate the deployment of your AI Platform Pipelines instance in the console before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: example TFX pipeline design pattern for Google Cloud\n",
    "The pipeline source code can be found in the `pipeline` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michal/PycharmProjects/tfx-titanic-training/pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd {notebook_path}/pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 180\r\n",
      "drwxrwxr-x 8 michal michal  4096 cze 16 17:14 .\r\n",
      "drwxrwxr-x 9 michal michal  4096 cze 16 22:51 ..\r\n",
      "-rw-rw-r-- 1 michal michal  8339 maj 27 17:06 airflow_runner.py\r\n",
      "-rw-rw-r-- 1 michal michal  5883 kwi 29 16:33 beam_dag_runner.py\r\n",
      "drwxrwxr-x 2 michal michal  4096 kwi 30 16:38 client\r\n",
      "-rw-rw-r-- 1 michal michal  3446 cze 16 00:51 config.py\r\n",
      "-rw-rw-r-- 1 michal michal    97 mar 10 12:14 Dockerfile\r\n",
      "-rw-rw-r-- 1 michal michal  4310 kwi  9 15:57 features.py\r\n",
      "drwxrwxr-x 2 michal michal  4096 kwi 30 16:38 hyperparameters\r\n",
      "-rw-rw-r-- 1 michal michal     0 kwi  9 11:32 __init__.py\r\n",
      "-rw-rw-r-- 1 michal michal  7075 cze 16 16:46 kubeflow_runner.py\r\n",
      "drwxrwxr-x 4 michal michal  4096 maj 29 15:05 lib\r\n",
      "-rw-rw-r-- 1 michal michal  5604 maj 29 15:08 local_runner.py\r\n",
      "-rw-rw-r-- 1 michal michal 15314 maj 21 19:02 model.py\r\n",
      "-rw-rw-r-- 1 michal michal  3452 maj 16 15:19 pipeline_args.py\r\n",
      "-rw-rw-r-- 1 michal michal 16049 maj 29 01:03 pipelines.py\r\n",
      "-rw-rw-r-- 1 michal michal  4506 kwi 11 02:00 preprocessing.py\r\n",
      "drwxrwxr-x 2 michal michal  4096 cze 16 17:14 __pycache__\r\n",
      "drwxrwxr-x 2 michal michal  4096 kwi 30 16:38 schema\r\n",
      "drwxrwxr-x 3 michal michal  4096 maj 21 18:52 tests\r\n",
      "-rw-rw-r-- 1 michal michal 52546 cze 16 01:16 tfx-titanic-training.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `config.py` module configures the default values for the environment specific settings and the default values for the pipeline runtime parameters. \n",
    "The default values can be overwritten at compile time by providing the updated values in a set of environment variables. You will set custom environment variables later on this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pipeline.py` module contains the TFX DSL defining the workflow implemented by the pipeline.\n",
    "\n",
    "The `preprocessing.py` module implements the data preprocessing logic  the `Transform` component.\n",
    "\n",
    "The `model.py` module implements the training, tuning, and model building logic for the `Trainer` and `Tuner` components.\n",
    "\n",
    "The `runner.py` module configures and executes `KubeflowDagRunner`. At compile time, the `KubeflowDagRunner.run()` method converts the TFX DSL into the pipeline package in the [argo](https://argoproj.github.io/argo/) format for execution on your hosted AI Platform Pipelines instance.\n",
    "\n",
    "The `features.py` module contains feature definitions common across `preprocessing.py` and `model.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: build your pipeline with the TFX CLI\n",
    "\n",
    "You will use TFX CLI to compile and deploy the pipeline. As explained in the previous section, the environment specific settings can be provided through a set of environment variables and embedded into the pipeline package at compile time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your environment resource settings\n",
    "\n",
    "Update  the below constants  with the settings reflecting your lab environment. \n",
    "\n",
    "- `GCP_REGION` - the compute region for AI Platform Training, Vizier, and Prediction.\n",
    "- `ARTIFACT_STORE` - An existing GCS bucket. You can use any bucket or use the GCS bucket created during installation of AI Platform Pipelines. The default bucket name will contain the `kubeflowpipelines-` prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://artifacts.cloud-training-281409.appspot.com/\r\n",
      "gs://cloud-training-281409/\r\n",
      "gs://cloud-training-281409-kubeflowpipelines-default/\r\n",
      "gs://kubeflow-storage-goose/\r\n"
     ]
    }
   ],
   "source": [
    "# Use the following command to identify the GCS bucket for metadata and pipeline storage.\n",
    "!gsutil ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `CUSTOM_SERVICE_ACCOUNT` - In the gcp console Click on the Navigation Menu. Navigate to `IAM & Admin`, then to `Service Accounts` and use the service account starting with prefix - `'tfx-tuner-caip-service-account'`. This enables CloudTuner and the Google Cloud AI Platform extensions Tuner component to work together and allows for distributed and parallel tuning backed by AI Platform Vizier's hyperparameter search algorithm. Please see the lab setup `README` for setup instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ENDPOINT` - set the `ENDPOINT` constant to the endpoint to your AI Platform Pipelines instance. The endpoint to the AI Platform Pipelines instance can be found on the [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page in the Google Cloud Console. Open the *SETTINGS* for your instance and use the value of the `host` variable in the *Connect to this Kubeflow Pipelines instance from a Python client via Kubeflow Pipelines SKD* section of the *SETTINGS* window. The format is `'...pipelines.googleusercontent.com'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCP run environment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Set your environment resource settings here for GCP_REGION, ARTIFACT_STORE_URI, ENDPOINT, and CUSTOM_SERVICE_ACCOUNT.\n",
    "GCP_REGION = 'us-central1'\n",
    "ARTIFACT_STORE_URI = 'gs://cloud-training-281409-kubeflowpipelines-default'\n",
    "ENDPOINT = 'https://c281ee5ff79be53-dot-us-central1.pipelines.googleusercontent.com'\n",
    "CUSTOM_SERVICE_ACCOUNT = 'tfx-tuner-service-account@cloud-training-281409.iam.gserviceaccount.com'\n",
    "\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "USE_GS=True\n",
    "USE_AI_PLATFORM=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GCP_REGION=us-central1\n",
      "env: ARTIFACT_STORE_URI=gs://cloud-training-281409-kubeflowpipelines-default\n",
      "env: CUSTOM_SERVICE_ACCOUNT=tfx-tuner-service-account@cloud-training-281409.iam.gserviceaccount.com\n",
      "env: PROJECT_ID=cloud-training-281409\n",
      "env: USE_GS=True\n",
      "env: USE_AI_PLATFORM=True\n"
     ]
    }
   ],
   "source": [
    "# Set your resource settings as environment variables. These override the default values in pipeline/config.py.\n",
    "%env GCP_REGION={GCP_REGION}\n",
    "%env ARTIFACT_STORE_URI={ARTIFACT_STORE_URI}\n",
    "%env CUSTOM_SERVICE_ACCOUNT={CUSTOM_SERVICE_ACCOUNT}\n",
    "%env PROJECT_ID={PROJECT_ID}\n",
    "\n",
    "%env USE_GS={USE_GS}\n",
    "%env USE_AI_PLATFORM={USE_AI_PLATFORM}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kubeflow run on local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "KUBEFLOW_ARTIFACT_STORE = os.path.join(os.sep, HOME, 'kubeflow-artifact-store')\n",
    "#ARTIFACT_STORE_URI = KUBEFLOW_ARTIFACT_STORE\n",
    "\n",
    "\n",
    "GCP_REGION = 'us-central1'\n",
    "ARTIFACT_STORE_URI = 'gs://cloud-training-281409-kubeflowpipelines-default'\n",
    "ENDPOINT = 'http://localhost:8080'\n",
    "CUSTOM_SERVICE_ACCOUNT = 'tfx-tuner-service-account@cloud-training-281409.iam.gserviceaccount.com'\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "USE_GS=True\n",
    "USE_AI_PLATFORM=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GCP_REGION=us-central1\n",
      "env: ARTIFACT_STORE_URI=gs://cloud-training-281409-kubeflowpipelines-default\n",
      "env: CUSTOM_SERVICE_ACCOUNT=tfx-tuner-service-account@cloud-training-281409.iam.gserviceaccount.com\n",
      "env: PROJECT_ID=cloud-training-281409\n",
      "env: USE_GS=True\n",
      "env: USE_AI_PLATFORM=False\n"
     ]
    }
   ],
   "source": [
    "# Set your resource settings as environment variables. These override the default values in pipeline/config.py.\n",
    "%env GCP_REGION={GCP_REGION}\n",
    "%env ARTIFACT_STORE_URI={ARTIFACT_STORE_URI}\n",
    "%env CUSTOM_SERVICE_ACCOUNT={CUSTOM_SERVICE_ACCOUNT}\n",
    "%env PROJECT_ID={PROJECT_ID}\n",
    "\n",
    "%env USE_GS={USE_GS}\n",
    "%env USE_AI_PLATFORM={USE_AI_PLATFORM}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the compile time settings to first create a pipeline version without hyperparameter tuning\n",
    "\n",
    "Default pipeline runtime environment values are configured in the pipeline folder `config.py`. You will set their values directly below:\n",
    "\n",
    "* `PIPELINE_NAME` - the pipeline's globally unique name. For each pipeline update, each pipeline version uploaded to KFP will be reflected on the `Pipelines` tab in the `Pipeline name > Version name` dropdown in the format `PIPELINE_NAME_datetime.now()`.\n",
    "\n",
    "* `MODEL_NAME` - the pipeline's unique model output name for AI Platform Prediction. For multiple pipeline runs, each pushed blessed model will create a new version with the format `'v{}'.format(int(time.time()))`.\n",
    "\n",
    "* `DATA_ROOT_URI` - the URI for the raw lab dataset `gs://workshop-datasets/covertype/small`.\n",
    "\n",
    "* `CUSTOM_TFX_IMAGE` - the image name of your pipeline container build by skaffold and published by `Cloud Build` to `Cloud Container Registry` in the format `'gcr.io/{}/{}'.format(PROJECT_ID, PIPELINE_NAME)`.\n",
    "\n",
    "* `RUNTIME_VERSION` - the TensorFlow runtime version. This lab was built and tested using TensorFlow `2.3`.\n",
    "\n",
    "* `PYTHON_VERSION` - the Python runtime version. This lab was built and tested using Python `3.7`.\n",
    "\n",
    "* `USE_KFP_SA` - The pipeline can run using a security context of the GKE default node pool's service account or the service account defined in the `user-gcp-sa` secret of the Kubernetes namespace hosting Kubeflow Pipelines. If you want to use the `user-gcp-sa` service account you change the value of `USE_KFP_SA` to `True`. Note that the default AI Platform Pipelines configuration does not define the `user-gcp-sa` secret.\n",
    "\n",
    "* `ENABLE_TUNING` - boolean value indicating whether to add the `Tuner` component to the pipeline or use hyperparameter defaults. See the `model.py` and `pipeline.py` files for details on how this changes the pipeline topology across pipeline versions. You will create pipeline versions without and with tuning enabled in the subsequent lab exercises for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_FOLDER = local_code_dirpath\n",
    "PIPELINE_NAME = 'tfx-titanic-training'\n",
    "MODEL_NAME = 'tfx_titanic-classifier'\n",
    "#DATA_ROOT_URI = local_train_dirpath\n",
    "DATA_ROOT_URI = 'gs://cloud-training-281409-kubeflowpipelines-default/tfx-template/data/titanic'\n",
    "\n",
    "RUNTIME_VERSION = '2.4'\n",
    "PYTHON_VERSION = '3.8'\n",
    "USE_KFP_SA=True\n",
    "ENABLE_CACHE=False\n",
    "\n",
    "EPOCHS = '50'\n",
    "TRAIN_STEPS = '2000'\n",
    "EVAL_STEPS = '100'\n",
    "TRAIN_BATCH_SIZE = '64'\n",
    "EVAL_BATCH_SIZE = '64'\n",
    "\n",
    "ENABLE_TUNING=True\n",
    "TUNER_STEPS = '400'\n",
    "MAX_TRIALS = '50'\n",
    "\n",
    "KUBEFLOW_TFX_IMAGE = 'tensorflow/tfx:0.30.0'\n",
    "\n",
    "CUSTOM_TFX_IMAGE = 'gcr.io/{}/{}'.format('local_kubeflow', PIPELINE_NAME)\n",
    "\n",
    "LOCAL_RUN = 'False'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CODE_FOLDER=/home/michal/PycharmProjects/tfx-titanic-training/pipeline\n",
      "env: PIPELINE_NAME=tfx-titanic-training\n",
      "env: MODEL_NAME=tfx_titanic-classifier\n",
      "env: DATA_ROOT_URI=gs://cloud-training-281409-kubeflowpipelines-default/tfx-template/data/titanic\n",
      "env: RUNTIME_VERSION=2.4\n",
      "env: PYTHON_VERSION=3.8\n",
      "env: USE_KFP_SA=True\n",
      "env: ENABLE_CACHE=False\n",
      "env: TRAIN_STEPS=2000\n",
      "env: EVAL_STEPS=100\n",
      "env: TRAIN_BATCH_SIZE=64\n",
      "env: EVAL_BATCH_SIZE=64\n",
      "env: ENABLE_TUNING=True\n",
      "env: TUNER_STEPS=400\n",
      "env: MAX_TRIALS=50\n",
      "env: KUBEFLOW_TFX_IMAGE=tensorflow/tfx:0.30.0\n",
      "env: CUSTOM_TFX_IMAGE=gcr.io/local_kubeflow/tfx-titanic-training\n",
      "env: LOCAL_RUN=False\n"
     ]
    }
   ],
   "source": [
    "%env CODE_FOLDER={CODE_FOLDER}\n",
    "%env PIPELINE_NAME={PIPELINE_NAME}\n",
    "%env MODEL_NAME={MODEL_NAME}\n",
    "%env DATA_ROOT_URI={DATA_ROOT_URI}\n",
    "\n",
    "%env RUNTIME_VERSION={RUNTIME_VERSION}\n",
    "%env PYTHON_VERSION={PYTHON_VERSION}\n",
    "%env USE_KFP_SA={USE_KFP_SA}\n",
    "%env ENABLE_CACHE={ENABLE_CACHE}\n",
    "\n",
    "%env TRAIN_STEPS={TRAIN_STEPS}\n",
    "%env EVAL_STEPS={EVAL_STEPS}\n",
    "%env TRAIN_BATCH_SIZE={TRAIN_BATCH_SIZE}\n",
    "%env EVAL_BATCH_SIZE={EVAL_BATCH_SIZE}\n",
    "\n",
    "\n",
    "%env ENABLE_TUNING={ENABLE_TUNING}\n",
    "%env TUNER_STEPS={TUNER_STEPS}\n",
    "%env MAX_TRIALS={MAX_TRIALS}\n",
    "\n",
    "%env KUBEFLOW_TFX_IMAGE={KUBEFLOW_TFX_IMAGE}\n",
    "%env CUSTOM_TFX_IMAGE={CUSTOM_TFX_IMAGE}\n",
    "\n",
    "%env LOCAL_RUN={LOCAL_RUN}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "_data_root = os.path.join(\".\", 'data')\n",
    "_train_dirpath = os.path.join(_data_root, \"train\")\n",
    "_train_filepath = os.path.join(_train_dirpath, \"train.csv\")\n",
    "_test_dirpath = os.path.join(_data_root, \"test\")\n",
    "_test_filepath = os.path.join(_test_dirpath, \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data from kaggle, unzip it and copy it to data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading titanic.zip to ./data\n",
      "  0%|                                               | 0.00/34.1k [00:00<?, ?B/s]\n",
      "100%|██████████████████████████████████████| 34.1k/34.1k [00:00<00:00, 6.56MB/s]\n",
      "Archive:  ./data/titanic.zip\n",
      "  inflating: ./data/gender_submission.csv  \n",
      "  inflating: ./data/test.csv         \n",
      "  inflating: ./data/train.csv        \n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c titanic -p {_data_root} --force\n",
    "!unzip -o {_data_root}/\"titanic.zip\" -d {_data_root}\n",
    "!cp {_data_root}/\"train.csv\" {_train_filepath}\n",
    "!cp {_data_root}/\"test.csv\" {_test_filepath}\n",
    "\n",
    "# clean up\n",
    "!rm  {_data_root}/*.csv  {_data_root}/*.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upload our sample data to GCS bucket so that we can use it in our pipeline later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy csv data to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Updates are available for some Cloud SDK components.  To install them,\n",
      "please run:\n",
      "  $ gcloud components update\n",
      "\n",
      "Copying file:///home/michal/PycharmProjects/tfx-titanic-training/data/train/train.csv [Content-Type=text/csv]...\n",
      "- [1 files][ 58.9 KiB/ 58.9 KiB]                                                \n",
      "Operation completed over 1 objects/58.9 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp {notebook_path}/data/train/train.csv gs://cloud-training-281409-kubeflowpipelines-default/tfx-template/data/titanic/data.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change directory to pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/michal/PycharmProjects/tfx-titanic-training/pipeline\n"
     ]
    }
   ],
   "source": [
    "%cd {notebook_path}/pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile your pipeline code\n",
    "\n",
    "You can build and upload the pipeline to the AI Platform Pipelines instance in one step, using the `tfx pipeline create` command. The `tfx pipeline create` goes through the following steps:\n",
    "- (Optional) Builds the custom image to that provides a runtime environment for TFX components or uses the latest image of the installed TFX version \n",
    "- Compiles the pipeline code into a pipeline package \n",
    "- Uploads the pipeline package via the `ENDPOINT` to the hosted AI Platform instance.\n",
    "\n",
    "As you debug the pipeline DSL, you may prefer to first use the `tfx pipeline compile` command, which only executes the compilation step. After the DSL compiles successfully you can use `tfx pipeline create` to go through all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-22 16:31:19.943129: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2021-06-22 16:31:19.943292: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "CLI\n",
      "Compiling pipeline\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "INFO:absl:PIPELINE_ROOT=gs://cloud-training-281409-kubeflowpipelines-default/tfx-titanic-training/{{workflow.uid}}\n",
      "INFO:absl:pipeline_name: tfx-titanic-training\n",
      "INFO:absl:pipeline root: gs://cloud-training-281409-kubeflowpipelines-default/tfx-titanic-training/{{workflow.uid}}\n",
      "INFO:absl:data_root_uri for training: gs://cloud-training-281409-kubeflowpipelines-default/tfx-template/data/titanic\n",
      "INFO:absl:train_steps for training: 2000\n",
      "INFO:absl:tuner_steps for tuning: 400\n",
      "INFO:absl:eval_steps for evaluating: 100\n",
      "INFO:absl:os default list dir: ['__init__.py', 'schema', 'local_runner.py', 'kubeflow_runner.py', 'tfx-titanic-training.tar.gz', 'pipelines.py', 'client', 'tests', 'model.py', 'hyperparameters', '__pycache__', 'beam_dag_runner.py', 'features.py', 'lib', 'preprocessing.py', 'airflow_runner.py', 'config.py', 'Dockerfile', 'pipeline_args.py']\n",
      "INFO:absl:schema_proper_folder: /home/michal/PycharmProjects/tfx-titanic-training/pipeline/schema\n",
      "INFO:absl:preprocessing_proper_file: /home/michal/PycharmProjects/tfx-titanic-training/pipeline/preprocessing.py\n",
      "INFO:absl:model_proper_file: /home/michal/PycharmProjects/tfx-titanic-training/pipeline/model.py\n",
      "INFO:absl:hyperparameters_proper_folder: /home/michal/PycharmProjects/tfx-titanic-training/pipeline/hyperparameters\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:train_steps: {\"__class__\": \"RuntimeParameter\", \"__module__\": \"tfx.orchestration.data_types\", \"__tfx_object_type__\": \"jsonable\", \"default\": 2000, \"description\": null, \"name\": \"train-steps\", \"ptype\": {\"__class__\": \"int\", \"__module__\": \"builtins\", \"__tfx_object_type__\": \"class\"}}\n",
      "INFO:absl:eval_steps: {\"__class__\": \"RuntimeParameter\", \"__module__\": \"tfx.orchestration.data_types\", \"__tfx_object_type__\": \"jsonable\", \"default\": 100, \"description\": null, \"name\": \"eval-steps\", \"ptype\": {\"__class__\": \"int\", \"__module__\": \"builtins\", \"__tfx_object_type__\": \"class\"}}\n",
      "INFO:absl:tuner_args: {'module_file': '/home/michal/PycharmProjects/tfx-titanic-training/pipeline/model.py', 'examples': Channel(\n",
      "    type_name: Examples\n",
      "    artifacts: []\n",
      "    additional_properties: {}\n",
      "    additional_custom_properties: {}\n",
      "), 'transform_graph': Channel(\n",
      "    type_name: TransformGraph\n",
      "    artifacts: []\n",
      "    additional_properties: {}\n",
      "    additional_custom_properties: {}\n",
      "), 'train_args': {'num_steps': 400}, 'eval_args': {'num_steps': 20}, 'custom_config': {'max_trials': 50, 'is_local_run': 0}}\n",
      "WARNING:absl:`custom_executor_spec` is going to be deprecated.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/bin/tfx\", line 8, in <module>\n",
      "    sys.exit(cli_group())\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 829, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 782, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 1259, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 1259, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 1066, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/decorators.py\", line 73, in new_func\n",
      "    return ctx.invoke(f, obj, *args, **kwargs)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/tfx/tools/cli/commands/pipeline.py\", line 316, in compile_pipeline\n",
      "    handler_factory.create_handler(ctx.flags_dict).compile_pipeline()\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/tfx/tools/cli/handler/kubeflow_handler.py\", line 143, in compile_pipeline\n",
      "    context = self.execute_dsl(patcher)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/tfx/tools/cli/handler/base_handler.py\", line 173, in execute_dsl\n",
      "    loader.exec_module(\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"kubeflow_runner.py\", line 168, in <module>\n",
      "    kubeflowRunner.run()\n",
      "  File \"kubeflow_runner.py\", line 143, in run\n",
      "    pipeline.create_pipeline(\n",
      "  File \"/home/michal/PycharmProjects/tfx-titanic-training/pipeline/pipelines.py\", line 356, in create_pipeline\n",
      "    pusher = Pusher(**pusher_args)  # pylint: disable=unused-variable\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/tfx/components/pusher/component.py\", line 105, in __init__\n",
      "    raise ValueError('push_destination is required unless a '\n",
      "ValueError: push_destination is required unless a custom_executor_spec is supplied that does not require it.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx pipeline compile --engine kubeflow --pipeline_path kubeflow_runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you should see a `{PIPELINE_NAME}.tar.gz` file appear in your current pipeline directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'next_page_token': None,\n",
       " 'pipelines': [{'created_at': datetime.datetime(2021, 6, 16, 20, 36, 12, tzinfo=tzutc()),\n",
       "                'default_version': {'code_source_url': None,\n",
       "                                    'created_at': datetime.datetime(2021, 6, 16, 20, 36, 12, tzinfo=tzutc()),\n",
       "                                    'id': '66c58f50-0330-4719-bd88-3aa472eb7994',\n",
       "                                    'name': '[Demo] XGBoost - Iterative model '\n",
       "                                            'training',\n",
       "                                    'package_url': None,\n",
       "                                    'parameters': None,\n",
       "                                    'resource_references': [{'key': {'id': '66c58f50-0330-4719-bd88-3aa472eb7994',\n",
       "                                                                     'type': 'PIPELINE'},\n",
       "                                                             'name': None,\n",
       "                                                             'relationship': 'OWNER'}]},\n",
       "                'description': '[source '\n",
       "                               'code](https://github.com/kubeflow/pipelines/blob/1c66f93f5149a8d5ed7f33895d3ebc01e662d837/samples/core/train_until_good/train_until_good.py) '\n",
       "                               'This sample demonstrates iterative training '\n",
       "                               'using a train-eval-check recursive loop. The '\n",
       "                               'main pipeline trains the initial model and '\n",
       "                               'then gradually trains the model some more '\n",
       "                               'until the model evaluation metrics are good '\n",
       "                               'enough.',\n",
       "                'error': None,\n",
       "                'id': '66c58f50-0330-4719-bd88-3aa472eb7994',\n",
       "                'name': '[Demo] XGBoost - Iterative model training',\n",
       "                'parameters': None,\n",
       "                'resource_references': None,\n",
       "                'url': None},\n",
       "               {'created_at': datetime.datetime(2021, 6, 16, 20, 36, 13, tzinfo=tzutc()),\n",
       "                'default_version': {'code_source_url': None,\n",
       "                                    'created_at': datetime.datetime(2021, 6, 16, 20, 36, 13, tzinfo=tzutc()),\n",
       "                                    'id': 'dceb15f2-e58f-497d-b329-08829fbb085a',\n",
       "                                    'name': '[Demo] TFX - Taxi tip prediction '\n",
       "                                            'model trainer',\n",
       "                                    'package_url': None,\n",
       "                                    'parameters': [{'name': 'pipeline-root',\n",
       "                                                    'value': 'gs://{{kfp-default-bucket}}/tfx_taxi_simple/{{workflow.uid}}'},\n",
       "                                                   {'name': 'data-root',\n",
       "                                                    'value': 'gs://ml-pipeline/sample-data/chicago-taxi/data'},\n",
       "                                                   {'name': 'module-file',\n",
       "                                                    'value': '/tfx/src/tfx/examples/chicago_taxi_pipeline/taxi_utils.py'}],\n",
       "                                    'resource_references': [{'key': {'id': 'dceb15f2-e58f-497d-b329-08829fbb085a',\n",
       "                                                                     'type': 'PIPELINE'},\n",
       "                                                             'name': None,\n",
       "                                                             'relationship': 'OWNER'}]},\n",
       "                'description': '[source '\n",
       "                               'code](https://github.com/kubeflow/pipelines/tree/1c66f93f5149a8d5ed7f33895d3ebc01e662d837/samples/core/parameterized_tfx_oss) '\n",
       "                               '[GCP Permission '\n",
       "                               'requirements](https://github.com/kubeflow/pipelines/blob/1c66f93f5149a8d5ed7f33895d3ebc01e662d837/samples/core/parameterized_tfx_oss#permission). '\n",
       "                               'Example pipeline that does classification with '\n",
       "                               'model analysis based on a public tax cab '\n",
       "                               'dataset.',\n",
       "                'error': None,\n",
       "                'id': 'dceb15f2-e58f-497d-b329-08829fbb085a',\n",
       "                'name': '[Demo] TFX - Taxi tip prediction model trainer',\n",
       "                'parameters': [{'name': 'pipeline-root',\n",
       "                                'value': 'gs://{{kfp-default-bucket}}/tfx_taxi_simple/{{workflow.uid}}'},\n",
       "                               {'name': 'data-root',\n",
       "                                'value': 'gs://ml-pipeline/sample-data/chicago-taxi/data'},\n",
       "                               {'name': 'module-file',\n",
       "                                'value': '/tfx/src/tfx/examples/chicago_taxi_pipeline/taxi_utils.py'}],\n",
       "                'resource_references': None,\n",
       "                'url': None},\n",
       "               {'created_at': datetime.datetime(2021, 6, 16, 20, 36, 14, tzinfo=tzutc()),\n",
       "                'default_version': {'code_source_url': None,\n",
       "                                    'created_at': datetime.datetime(2021, 6, 16, 20, 36, 14, tzinfo=tzutc()),\n",
       "                                    'id': 'c42d81fa-da8d-4108-98ff-df57d9c5f384',\n",
       "                                    'name': '[Tutorial] Data passing in python '\n",
       "                                            'components',\n",
       "                                    'package_url': None,\n",
       "                                    'parameters': None,\n",
       "                                    'resource_references': [{'key': {'id': 'c42d81fa-da8d-4108-98ff-df57d9c5f384',\n",
       "                                                                     'type': 'PIPELINE'},\n",
       "                                                             'name': None,\n",
       "                                                             'relationship': 'OWNER'}]},\n",
       "                'description': '[source '\n",
       "                               'code](https://github.com/kubeflow/pipelines/tree/1c66f93f5149a8d5ed7f33895d3ebc01e662d837/samples/tutorials/Data%20passing%20in%20python%20components) '\n",
       "                               'Shows how to pass data between python '\n",
       "                               'components.',\n",
       "                'error': None,\n",
       "                'id': 'c42d81fa-da8d-4108-98ff-df57d9c5f384',\n",
       "                'name': '[Tutorial] Data passing in python components',\n",
       "                'parameters': None,\n",
       "                'resource_references': None,\n",
       "                'url': None},\n",
       "               {'created_at': datetime.datetime(2021, 6, 16, 20, 36, 15, tzinfo=tzutc()),\n",
       "                'default_version': {'code_source_url': None,\n",
       "                                    'created_at': datetime.datetime(2021, 6, 16, 20, 36, 15, tzinfo=tzutc()),\n",
       "                                    'id': '1a0f8dbc-a7fe-4361-9d08-117239895a4a',\n",
       "                                    'name': '[Tutorial] DSL - Control '\n",
       "                                            'structures',\n",
       "                                    'package_url': None,\n",
       "                                    'parameters': None,\n",
       "                                    'resource_references': [{'key': {'id': '1a0f8dbc-a7fe-4361-9d08-117239895a4a',\n",
       "                                                                     'type': 'PIPELINE'},\n",
       "                                                             'name': None,\n",
       "                                                             'relationship': 'OWNER'}]},\n",
       "                'description': '[source '\n",
       "                               'code](https://github.com/kubeflow/pipelines/tree/1c66f93f5149a8d5ed7f33895d3ebc01e662d837/samples/tutorials/DSL%20-%20Control%20structures) '\n",
       "                               'Shows how to use conditional execution and '\n",
       "                               'exit handlers. This pipeline will randomly '\n",
       "                               'fail to demonstrate that the exit handler gets '\n",
       "                               'executed even in case of failure.',\n",
       "                'error': None,\n",
       "                'id': '1a0f8dbc-a7fe-4361-9d08-117239895a4a',\n",
       "                'name': '[Tutorial] DSL - Control structures',\n",
       "                'parameters': None,\n",
       "                'resource_references': None,\n",
       "                'url': None},\n",
       "               {'created_at': datetime.datetime(2021, 6, 16, 20, 54, 53, tzinfo=tzutc()),\n",
       "                'default_version': {'code_source_url': None,\n",
       "                                    'created_at': datetime.datetime(2021, 6, 18, 11, 22, 59, tzinfo=tzutc()),\n",
       "                                    'id': 'bb3fdeab-c5e1-47f1-ad65-97647f197530',\n",
       "                                    'name': 'tfx-titanic-training_20210618132259',\n",
       "                                    'package_url': None,\n",
       "                                    'parameters': [{'name': 'pipeline-root',\n",
       "                                                    'value': 'gs://cloud-training-281409-kubeflowpipelines-default/tfx-titanic-training/{{workflow.uid}}'},\n",
       "                                                   {'name': 'data_root',\n",
       "                                                    'value': 'gs://cloud-training-281409-kubeflowpipelines-default/tfx-template/data/titanic'},\n",
       "                                                   {'name': 'eval-steps',\n",
       "                                                    'value': '100'},\n",
       "                                                   {'name': 'train-steps',\n",
       "                                                    'value': '2000'}],\n",
       "                                    'resource_references': [{'key': {'id': '8670e379-208e-4a6c-8c80-3b1d78558680',\n",
       "                                                                     'type': 'PIPELINE'},\n",
       "                                                             'name': None,\n",
       "                                                             'relationship': 'OWNER'}]},\n",
       "                'description': None,\n",
       "                'error': None,\n",
       "                'id': '8670e379-208e-4a6c-8c80-3b1d78558680',\n",
       "                'name': 'tfx-titanic-training',\n",
       "                'parameters': [{'name': 'pipeline-root',\n",
       "                                'value': '/home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}'},\n",
       "                               {'name': 'data_root',\n",
       "                                'value': '/home/michal/PycharmProjects/tfx-titanic-training/data/train'},\n",
       "                               {'name': 'eval-steps', 'value': '100'},\n",
       "                               {'name': 'train-steps', 'value': '2000'}],\n",
       "                'resource_references': None,\n",
       "                'url': None}],\n",
       " 'total_size': 5}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kfp\n",
    "\n",
    "#ENDPOINT = 'http://localhost:8080'\n",
    "#ENDPOINT = 'http://localhost:8080'\n",
    "\n",
    "client = kfp.Client(host=ENDPOINT)\n",
    "\n",
    "client.list_pipelines()\n",
    "\n",
    "#client.KUBE_PROXY_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: deploy your pipeline container to AI Platform Pipelines with TFX CLI\n",
    "\n",
    "After the pipeline code compiles without any errors you can use the `tfx pipeline create` command to perform the full build and deploy the pipeline. You will deploy your compiled pipeline container hosted on Google Container Registry e.g. `gcr.io/[PROJECT_ID]/tfx_covertype_continuous_training` to run on AI Platform Pipelines with the TFX CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-16 22:54:45.795373: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2021-06-16 22:54:45.795424: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "CLI\n",
      "Creating pipeline\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "INFO:absl:PIPELINE_ROOT=/home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}\n",
      "INFO:absl:pipeline_name: tfx-titanic-training\n",
      "INFO:absl:pipeline root: /home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}\n",
      "INFO:absl:data_root_uri for training: /home/michal/PycharmProjects/tfx-titanic-training/data/train\n",
      "INFO:absl:train_steps for training: 2000\n",
      "INFO:absl:tuner_steps for tuning: 400\n",
      "INFO:absl:eval_steps for evaluating: 100\n",
      "INFO:absl:os default list dir: ['__init__.py', 'schema', 'local_runner.py', 'kubeflow_runner.py', 'tfx-titanic-training.tar.gz', 'pipelines.py', 'client', 'tests', 'model.py', 'hyperparameters', '__pycache__', 'beam_dag_runner.py', 'features.py', 'lib', 'preprocessing.py', 'airflow_runner.py', 'config.py', 'Dockerfile', 'pipeline_args.py']\n",
      "INFO:absl:schema_proper_folder: /home/michal/PycharmProjects/tfx-titanic-training/pipeline/schema\n",
      "INFO:absl:preprocessing_proper_file: /home/michal/PycharmProjects/tfx-titanic-training/pipeline/preprocessing.py\n",
      "INFO:absl:model_proper_file: /home/michal/PycharmProjects/tfx-titanic-training/pipeline/model.py\n",
      "INFO:absl:hyperparameters_proper_folder: /home/michal/PycharmProjects/tfx-titanic-training/pipeline/hyperparameters\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:train_steps: {\"__class__\": \"RuntimeParameter\", \"__module__\": \"tfx.orchestration.data_types\", \"__tfx_object_type__\": \"jsonable\", \"default\": 2000, \"description\": null, \"name\": \"train-steps\", \"ptype\": {\"__class__\": \"int\", \"__module__\": \"builtins\", \"__tfx_object_type__\": \"class\"}}\n",
      "INFO:absl:eval_steps: {\"__class__\": \"RuntimeParameter\", \"__module__\": \"tfx.orchestration.data_types\", \"__tfx_object_type__\": \"jsonable\", \"default\": 100, \"description\": null, \"name\": \"eval-steps\", \"ptype\": {\"__class__\": \"int\", \"__module__\": \"builtins\", \"__tfx_object_type__\": \"class\"}}\n",
      "INFO:absl:tuner_args: {'module_file': '/home/michal/PycharmProjects/tfx-titanic-training/pipeline/model.py', 'examples': Channel(\n",
      "    type_name: Examples\n",
      "    artifacts: []\n",
      "    additional_properties: {}\n",
      "    additional_custom_properties: {}\n",
      "), 'transform_graph': Channel(\n",
      "    type_name: TransformGraph\n",
      "    artifacts: []\n",
      "    additional_properties: {}\n",
      "    additional_custom_properties: {}\n",
      "), 'train_args': {'num_steps': 400}, 'eval_args': {'num_steps': 20}, 'custom_config': {'max_trials': 50, 'is_local_run': 1}}\n",
      "WARNING:absl:`custom_executor_spec` is going to be deprecated.\n",
      "INFO:absl:Generating ephemeral wheel package for '/home/michal/PycharmProjects/tfx-titanic-training/pipeline/preprocessing.py' (including modules: ['local_runner', 'kubeflow_runner', 'pipelines', 'model', 'beam_dag_runner', 'features', 'preprocessing', 'airflow_runner', 'config', 'pipeline_args']).\n",
      "INFO:absl:User module package has hash fingerprint version 0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.\n",
      "INFO:absl:Executing: ['/home/michal/venv/ML-tfx-0.30.0/bin/python3', '/tmp/tmpzc25ignl/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpyf5etbue', '--dist-dir', '/tmp/tmpfl2uo9ak']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying local_runner.py -> build/lib\n",
      "copying kubeflow_runner.py -> build/lib\n",
      "copying pipelines.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "copying beam_dag_runner.py -> build/lib\n",
      "copying features.py -> build/lib\n",
      "copying preprocessing.py -> build/lib\n",
      "copying airflow_runner.py -> build/lib\n",
      "copying config.py -> build/lib\n",
      "copying pipeline_args.py -> build/lib\n",
      "installing to /tmp/tmpyf5etbue\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/local_runner.py -> /tmp/tmpyf5etbue\n",
      "copying build/lib/kubeflow_runner.py -> /tmp/tmpyf5etbue\n",
      "copying build/lib/pipelines.py -> /tmp/tmpyf5etbue\n",
      "copying build/lib/model.py -> /tmp/tmpyf5etbue\n",
      "copying build/lib/beam_dag_runner.py -> /tmp/tmpyf5etbue\n",
      "copying build/lib/features.py -> /tmp/tmpyf5etbue\n",
      "copying build/lib/preprocessing.py -> /tmp/tmpyf5etbue\n",
      "copying build/lib/airflow_runner.py -> /tmp/tmpyf5etbue\n",
      "copying build/lib/config.py -> /tmp/tmpyf5etbue\n",
      "copying build/lib/pipeline_args.py -> /tmp/tmpyf5etbue\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Transform.egg-info\n",
      "writing tfx_user_code_Transform.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Transform.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Transform.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Transform.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Transform.egg-info to /tmp/tmpyf5etbue/tfx_user_code_Transform-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpyf5etbue/tfx_user_code_Transform-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/WHEEL\n",
      "creating '/tmp/tmpfl2uo9ak/tfx_user_code_Transform-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157-py3-none-any.whl' and adding '/tmp/tmpyf5etbue' to it\n",
      "adding 'airflow_runner.py'\n",
      "adding 'beam_dag_runner.py'\n",
      "adding 'config.py'\n",
      "adding 'features.py'\n",
      "adding 'kubeflow_runner.py'\n",
      "adding 'local_runner.py'\n",
      "adding 'model.py'\n",
      "adding 'pipeline_args.py'\n",
      "adding 'pipelines.py'\n",
      "adding 'preprocessing.py'\n",
      "adding 'tfx_user_code_Transform-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Transform-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Transform-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Transform-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/RECORD'\n",
      "removing /tmp/tmpyf5etbue\n",
      "INFO:absl:Successfully built user code wheel distribution at '/home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}/_wheels/tfx_user_code_Transform-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157-py3-none-any.whl'; target user module is 'preprocessing'.\n",
      "INFO:absl:Full user module path is 'preprocessing@/home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}/_wheels/tfx_user_code_Transform-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157-py3-none-any.whl'\n",
      "INFO:absl:Generating ephemeral wheel package for '/home/michal/PycharmProjects/tfx-titanic-training/pipeline/model.py' (including modules: ['local_runner', 'kubeflow_runner', 'pipelines', 'model', 'beam_dag_runner', 'features', 'preprocessing', 'airflow_runner', 'config', 'pipeline_args']).\n",
      "INFO:absl:User module package has hash fingerprint version 0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.\n",
      "INFO:absl:Executing: ['/home/michal/venv/ML-tfx-0.30.0/bin/python3', '/tmp/tmphz5tl7ki/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmpseuch2g3', '--dist-dir', '/tmp/tmp1hggifi0']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying local_runner.py -> build/lib\n",
      "copying kubeflow_runner.py -> build/lib\n",
      "copying pipelines.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "copying beam_dag_runner.py -> build/lib\n",
      "copying features.py -> build/lib\n",
      "copying preprocessing.py -> build/lib\n",
      "copying airflow_runner.py -> build/lib\n",
      "copying config.py -> build/lib\n",
      "copying pipeline_args.py -> build/lib\n",
      "installing to /tmp/tmpseuch2g3\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/local_runner.py -> /tmp/tmpseuch2g3\n",
      "copying build/lib/kubeflow_runner.py -> /tmp/tmpseuch2g3\n",
      "copying build/lib/pipelines.py -> /tmp/tmpseuch2g3\n",
      "copying build/lib/model.py -> /tmp/tmpseuch2g3\n",
      "copying build/lib/beam_dag_runner.py -> /tmp/tmpseuch2g3\n",
      "copying build/lib/features.py -> /tmp/tmpseuch2g3\n",
      "copying build/lib/preprocessing.py -> /tmp/tmpseuch2g3\n",
      "copying build/lib/airflow_runner.py -> /tmp/tmpseuch2g3\n",
      "copying build/lib/config.py -> /tmp/tmpseuch2g3\n",
      "copying build/lib/pipeline_args.py -> /tmp/tmpseuch2g3\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Tuner.egg-info\n",
      "writing tfx_user_code_Tuner.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Tuner.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Tuner.egg-info/top_level.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing manifest file 'tfx_user_code_Tuner.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Tuner.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Tuner.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Tuner.egg-info to /tmp/tmpseuch2g3/tfx_user_code_Tuner-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmpseuch2g3/tfx_user_code_Tuner-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/WHEEL\n",
      "creating '/tmp/tmp1hggifi0/tfx_user_code_Tuner-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157-py3-none-any.whl' and adding '/tmp/tmpseuch2g3' to it\n",
      "adding 'airflow_runner.py'\n",
      "adding 'beam_dag_runner.py'\n",
      "adding 'config.py'\n",
      "adding 'features.py'\n",
      "adding 'kubeflow_runner.py'\n",
      "adding 'local_runner.py'\n",
      "adding 'model.py'\n",
      "adding 'pipeline_args.py'\n",
      "adding 'pipelines.py'\n",
      "adding 'preprocessing.py'\n",
      "adding 'tfx_user_code_Tuner-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Tuner-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Tuner-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Tuner-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/RECORD'\n",
      "removing /tmp/tmpseuch2g3\n",
      "INFO:absl:Successfully built user code wheel distribution at '/home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}/_wheels/tfx_user_code_Tuner-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157-py3-none-any.whl'; target user module is 'model'.\n",
      "INFO:absl:Full user module path is 'model@/home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}/_wheels/tfx_user_code_Tuner-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157-py3-none-any.whl'\n",
      "INFO:absl:Generating ephemeral wheel package for '/home/michal/PycharmProjects/tfx-titanic-training/pipeline/model.py' (including modules: ['local_runner', 'kubeflow_runner', 'pipelines', 'model', 'beam_dag_runner', 'features', 'preprocessing', 'airflow_runner', 'config', 'pipeline_args']).\n",
      "INFO:absl:User module package has hash fingerprint version 0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.\n",
      "INFO:absl:Executing: ['/home/michal/venv/ML-tfx-0.30.0/bin/python3', '/tmp/tmpb271yh59/_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', '/tmp/tmp_cdnfkq1', '--dist-dir', '/tmp/tmp4hzxodfn']\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "copying local_runner.py -> build/lib\n",
      "copying kubeflow_runner.py -> build/lib\n",
      "copying pipelines.py -> build/lib\n",
      "copying model.py -> build/lib\n",
      "copying beam_dag_runner.py -> build/lib\n",
      "copying features.py -> build/lib\n",
      "copying preprocessing.py -> build/lib\n",
      "copying airflow_runner.py -> build/lib\n",
      "copying config.py -> build/lib\n",
      "copying pipeline_args.py -> build/lib\n",
      "installing to /tmp/tmp_cdnfkq1\n",
      "running install\n",
      "running install_lib\n",
      "copying build/lib/local_runner.py -> /tmp/tmp_cdnfkq1\n",
      "copying build/lib/kubeflow_runner.py -> /tmp/tmp_cdnfkq1\n",
      "copying build/lib/pipelines.py -> /tmp/tmp_cdnfkq1\n",
      "copying build/lib/model.py -> /tmp/tmp_cdnfkq1\n",
      "copying build/lib/beam_dag_runner.py -> /tmp/tmp_cdnfkq1\n",
      "copying build/lib/features.py -> /tmp/tmp_cdnfkq1\n",
      "copying build/lib/preprocessing.py -> /tmp/tmp_cdnfkq1\n",
      "copying build/lib/airflow_runner.py -> /tmp/tmp_cdnfkq1\n",
      "copying build/lib/config.py -> /tmp/tmp_cdnfkq1\n",
      "copying build/lib/pipeline_args.py -> /tmp/tmp_cdnfkq1\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating tfx_user_code_Trainer.egg-info\n",
      "writing tfx_user_code_Trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to tfx_user_code_Trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to tfx_user_code_Trainer.egg-info/top_level.txt\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "reading manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'tfx_user_code_Trainer.egg-info/SOURCES.txt'\n",
      "Copying tfx_user_code_Trainer.egg-info to /tmp/tmp_cdnfkq1/tfx_user_code_Trainer-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.egg-info\n",
      "running install_scripts\n",
      "creating /tmp/tmp_cdnfkq1/tfx_user_code_Trainer-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/WHEEL\n",
      "creating '/tmp/tmp4hzxodfn/tfx_user_code_Trainer-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157-py3-none-any.whl' and adding '/tmp/tmp_cdnfkq1' to it\n",
      "adding 'airflow_runner.py'\n",
      "adding 'beam_dag_runner.py'\n",
      "adding 'config.py'\n",
      "adding 'features.py'\n",
      "adding 'kubeflow_runner.py'\n",
      "adding 'local_runner.py'\n",
      "adding 'model.py'\n",
      "adding 'pipeline_args.py'\n",
      "adding 'pipelines.py'\n",
      "adding 'preprocessing.py'\n",
      "adding 'tfx_user_code_Trainer-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/METADATA'\n",
      "adding 'tfx_user_code_Trainer-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/WHEEL'\n",
      "adding 'tfx_user_code_Trainer-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/top_level.txt'\n",
      "adding 'tfx_user_code_Trainer-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157.dist-info/RECORD'\n",
      "removing /tmp/tmp_cdnfkq1\n",
      "INFO:absl:Successfully built user code wheel distribution at '/home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}/_wheels/tfx_user_code_Trainer-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157-py3-none-any.whl'; target user module is 'model'.\n",
      "INFO:absl:Full user module path is 'model@/home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}/_wheels/tfx_user_code_Trainer-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157-py3-none-any.whl'\n",
      "INFO:absl:Generated pipeline:\n",
      " pipeline_info {\n",
      "  id: \"tfx-titanic-training\"\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "      }\n",
      "      id: \"CsvExampleGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"tfx-titanic-training\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"tfx-titanic-training.CsvExampleGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Examples\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "              properties {\n",
      "                key: \"version\"\n",
      "                value: INT\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"input_base\"\n",
      "        value {\n",
      "          runtime_parameter {\n",
      "            name: \"data_root\"\n",
      "            type: STRING\n",
      "            default_value {\n",
      "              string_value: \"/home/michal/PycharmProjects/tfx-titanic-training/data/train\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"input_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 4,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_data_format\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 6\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"Evaluator\"\n",
      "    downstream_nodes: \"InfraValidator\"\n",
      "    downstream_nodes: \"StatisticsGen\"\n",
      "    downstream_nodes: \"Transform\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.dsl.components.common.importer.Importer\"\n",
      "      }\n",
      "      id: \"import_user_schema\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"tfx-titanic-training\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"tfx-titanic-training.import_user_schema\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"result\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Schema\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"artifact_uri\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"/home/michal/PycharmProjects/tfx-titanic-training/pipeline/schema\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"reimport\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 0\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"ExampleValidator\"\n",
      "    downstream_nodes: \"Trainer\"\n",
      "    downstream_nodes: \"Transform\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "      }\n",
      "      id: \"latest_blessed_model_resolver\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"tfx-titanic-training\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"tfx-titanic-training.latest_blessed_model_resolver\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          channels {\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"tfx-titanic-training\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"model_blessing\"\n",
      "        value {\n",
      "          channels {\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"tfx-titanic-training\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ModelBlessing\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      resolver_config {\n",
      "        resolver_steps {\n",
      "          class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "          config_json: \"{}\"\n",
      "          input_keys: \"model\"\n",
      "          input_keys: \"model_blessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"Evaluator\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "      }\n",
      "      id: \"StatisticsGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"tfx-titanic-training\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"tfx-titanic-training.StatisticsGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"CsvExampleGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"tfx-titanic-training\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"tfx-titanic-training.CsvExampleGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"statistics\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ExampleStatistics\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"exclude_splits\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"[]\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"CsvExampleGen\"\n",
      "    downstream_nodes: \"ExampleValidator\"\n",
      "    downstream_nodes: \"SchemaGen\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.transform.component.Transform\"\n",
      "      }\n",
      "      id: \"Transform\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"tfx-titanic-training\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"tfx-titanic-training.Transform\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"CsvExampleGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"tfx-titanic-training\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"tfx-titanic-training.CsvExampleGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"import_user_schema\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"tfx-titanic-training\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"tfx-titanic-training.import_user_schema\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Schema\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"result\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"transform_graph\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"TransformGraph\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"transformed_examples\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Examples\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "              properties {\n",
      "                key: \"version\"\n",
      "                value: INT\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"updated_analyzer_cache\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"TransformCache\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"custom_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"force_tf_compat_v1\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 0\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"module_path\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"preprocessing@/home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}/_wheels/tfx_user_code_Transform-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157-py3-none-any.whl\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"CsvExampleGen\"\n",
      "    upstream_nodes: \"import_user_schema\"\n",
      "    downstream_nodes: \"Trainer\"\n",
      "    downstream_nodes: \"Tuner\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "      }\n",
      "      id: \"ExampleValidator\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"tfx-titanic-training\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"tfx-titanic-training.ExampleValidator\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"import_user_schema\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"tfx-titanic-training\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"tfx-titanic-training.import_user_schema\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Schema\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"result\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"statistics\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"StatisticsGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"tfx-titanic-training\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"tfx-titanic-training.StatisticsGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ExampleStatistics\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"statistics\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"anomalies\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ExampleAnomalies\"\n",
      "              properties {\n",
      "                key: \"span\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                value: INT\r\n",
      "              }\r\n",
      "              properties {\r\n",
      "                key: \"split_names\"\r\n",
      "                value: STRING\r\n",
      "              }\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    parameters {\r\n",
      "      parameters {\r\n",
      "        key: \"exclude_splits\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"[]\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    upstream_nodes: \"StatisticsGen\"\r\n",
      "    upstream_nodes: \"import_user_schema\"\r\n",
      "    execution_options {\r\n",
      "      caching_options {\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "nodes {\r\n",
      "  pipeline_node {\r\n",
      "    node_info {\r\n",
      "      type {\r\n",
      "        name: \"tfx.components.schema_gen.component.SchemaGen\"\r\n",
      "      }\r\n",
      "      id: \"SchemaGen\"\r\n",
      "    }\r\n",
      "    contexts {\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"pipeline\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          field_value {\r\n",
      "            string_value: \"tfx-titanic-training\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"pipeline_run\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          runtime_parameter {\r\n",
      "            name: \"pipeline_run_id\"\r\n",
      "            type: STRING\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"node\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          field_value {\r\n",
      "            string_value: \"tfx-titanic-training.SchemaGen\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    inputs {\r\n",
      "      inputs {\r\n",
      "        key: \"statistics\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"StatisticsGen\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.StatisticsGen\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"ExampleStatistics\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"statistics\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    outputs {\r\n",
      "      outputs {\r\n",
      "        key: \"schema\"\r\n",
      "        value {\r\n",
      "          artifact_spec {\r\n",
      "            type {\r\n",
      "              name: \"Schema\"\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    parameters {\r\n",
      "      parameters {\r\n",
      "        key: \"exclude_splits\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"[]\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      parameters {\r\n",
      "        key: \"infer_feature_shape\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            int_value: 1\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    upstream_nodes: \"StatisticsGen\"\r\n",
      "    execution_options {\r\n",
      "      caching_options {\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "nodes {\r\n",
      "  pipeline_node {\r\n",
      "    node_info {\r\n",
      "      type {\r\n",
      "        name: \"tfx.components.tuner.component.Tuner\"\r\n",
      "      }\r\n",
      "      id: \"Tuner\"\r\n",
      "    }\r\n",
      "    contexts {\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"pipeline\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          field_value {\r\n",
      "            string_value: \"tfx-titanic-training\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"pipeline_run\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          runtime_parameter {\r\n",
      "            name: \"pipeline_run_id\"\r\n",
      "            type: STRING\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"node\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          field_value {\r\n",
      "            string_value: \"tfx-titanic-training.Tuner\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    inputs {\r\n",
      "      inputs {\r\n",
      "        key: \"examples\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"Transform\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.Transform\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"Examples\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"transformed_examples\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      inputs {\r\n",
      "        key: \"transform_graph\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"Transform\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.Transform\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"TransformGraph\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"transform_graph\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    outputs {\r\n",
      "      outputs {\r\n",
      "        key: \"best_hyperparameters\"\r\n",
      "        value {\r\n",
      "          artifact_spec {\r\n",
      "            type {\r\n",
      "              name: \"HyperParameters\"\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    parameters {\r\n",
      "      parameters {\r\n",
      "        key: \"custom_config\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"{\\\"is_local_run\\\": 1, \\\"max_trials\\\": 50}\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      parameters {\r\n",
      "        key: \"eval_args\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"{\\\"num_steps\\\": 20}\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      parameters {\r\n",
      "        key: \"module_path\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"model@/home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}/_wheels/tfx_user_code_Tuner-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157-py3-none-any.whl\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      parameters {\r\n",
      "        key: \"train_args\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"{\\\"num_steps\\\": 400}\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    upstream_nodes: \"Transform\"\r\n",
      "    downstream_nodes: \"Trainer\"\r\n",
      "    execution_options {\r\n",
      "      caching_options {\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "nodes {\r\n",
      "  pipeline_node {\r\n",
      "    node_info {\r\n",
      "      type {\r\n",
      "        name: \"tfx.components.trainer.component.Trainer\"\r\n",
      "      }\r\n",
      "      id: \"Trainer\"\r\n",
      "    }\r\n",
      "    contexts {\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"pipeline\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          field_value {\r\n",
      "            string_value: \"tfx-titanic-training\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"pipeline_run\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          runtime_parameter {\r\n",
      "            name: \"pipeline_run_id\"\r\n",
      "            type: STRING\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"node\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          field_value {\r\n",
      "            string_value: \"tfx-titanic-training.Trainer\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    inputs {\r\n",
      "      inputs {\r\n",
      "        key: \"examples\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"Transform\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.Transform\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"Examples\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"transformed_examples\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      inputs {\r\n",
      "        key: \"hyperparameters\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"Tuner\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.Tuner\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"HyperParameters\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"best_hyperparameters\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      inputs {\r\n",
      "        key: \"schema\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"import_user_schema\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.import_user_schema\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"Schema\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"result\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      inputs {\r\n",
      "        key: \"transform_graph\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"Transform\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.Transform\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"TransformGraph\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"transform_graph\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    outputs {\r\n",
      "      outputs {\r\n",
      "        key: \"model\"\r\n",
      "        value {\r\n",
      "          artifact_spec {\r\n",
      "            type {\r\n",
      "              name: \"Model\"\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      outputs {\r\n",
      "        key: \"model_run\"\r\n",
      "        value {\r\n",
      "          artifact_spec {\r\n",
      "            type {\r\n",
      "              name: \"ModelRun\"\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    parameters {\r\n",
      "      parameters {\r\n",
      "        key: \"custom_config\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"{\\\"epochs\\\": 10, \\\"eval_batch_size\\\": 64, \\\"train_batch_size\\\": 64}\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      parameters {\r\n",
      "        key: \"eval_args\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"{\\\"num_steps\\\": {\\\"__class__\\\": \\\"RuntimeParameter\\\", \\\"__module__\\\": \\\"tfx.orchestration.data_types\\\", \\\"__tfx_object_type__\\\": \\\"jsonable\\\", \\\"default\\\": 100, \\\"description\\\": null, \\\"name\\\": \\\"eval-steps\\\", \\\"ptype\\\": {\\\"__class__\\\": \\\"int\\\", \\\"__module__\\\": \\\"builtins\\\", \\\"__tfx_object_type__\\\": \\\"class\\\"}}}\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      parameters {\r\n",
      "        key: \"module_path\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"model@/home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}/_wheels/tfx_user_code_Trainer-0.0+0a6a320852b6ce12f52a47d741a665a38c58498e157bb473a99d923bc4e7f157-py3-none-any.whl\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      parameters {\r\n",
      "        key: \"train_args\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"{\\\"num_steps\\\": {\\\"__class__\\\": \\\"RuntimeParameter\\\", \\\"__module__\\\": \\\"tfx.orchestration.data_types\\\", \\\"__tfx_object_type__\\\": \\\"jsonable\\\", \\\"default\\\": 2000, \\\"description\\\": null, \\\"name\\\": \\\"train-steps\\\", \\\"ptype\\\": {\\\"__class__\\\": \\\"int\\\", \\\"__module__\\\": \\\"builtins\\\", \\\"__tfx_object_type__\\\": \\\"class\\\"}}}\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    upstream_nodes: \"Transform\"\r\n",
      "    upstream_nodes: \"Tuner\"\r\n",
      "    upstream_nodes: \"import_user_schema\"\r\n",
      "    downstream_nodes: \"Evaluator\"\r\n",
      "    downstream_nodes: \"InfraValidator\"\r\n",
      "    downstream_nodes: \"Pusher\"\r\n",
      "    execution_options {\r\n",
      "      caching_options {\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "nodes {\r\n",
      "  pipeline_node {\r\n",
      "    node_info {\r\n",
      "      type {\r\n",
      "        name: \"tfx.components.evaluator.component.Evaluator\"\r\n",
      "      }\r\n",
      "      id: \"Evaluator\"\r\n",
      "    }\r\n",
      "    contexts {\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"pipeline\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          field_value {\r\n",
      "            string_value: \"tfx-titanic-training\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"pipeline_run\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          runtime_parameter {\r\n",
      "            name: \"pipeline_run_id\"\r\n",
      "            type: STRING\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"node\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          field_value {\r\n",
      "            string_value: \"tfx-titanic-training.Evaluator\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    inputs {\r\n",
      "      inputs {\r\n",
      "        key: \"baseline_model\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"latest_blessed_model_resolver\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.latest_blessed_model_resolver\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"Model\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"model\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      inputs {\r\n",
      "        key: \"examples\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"CsvExampleGen\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.CsvExampleGen\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"Examples\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"examples\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      inputs {\r\n",
      "        key: \"model\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"Trainer\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.Trainer\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"Model\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"model\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    outputs {\r\n",
      "      outputs {\r\n",
      "        key: \"blessing\"\r\n",
      "        value {\r\n",
      "          artifact_spec {\r\n",
      "            type {\r\n",
      "              name: \"ModelBlessing\"\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      outputs {\r\n",
      "        key: \"evaluation\"\r\n",
      "        value {\r\n",
      "          artifact_spec {\r\n",
      "            type {\r\n",
      "              name: \"ModelEvaluation\"\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    parameters {\r\n",
      "      parameters {\r\n",
      "        key: \"eval_config\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.5,\\n              \\\"upper_bound\\\": 0.995\\n            }\\n          }\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"Survived\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {},\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"Sex\\\"\\n      ]\\n    },\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"Age\\\"\\n      ]\\n    },\\n    {\\n      \\\"feature_keys\\\": [\\n        \\\"Parch\\\"\\n      ]\\n    }\\n  ]\\n}\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      parameters {\r\n",
      "        key: \"example_splits\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"null\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    upstream_nodes: \"CsvExampleGen\"\r\n",
      "    upstream_nodes: \"Trainer\"\r\n",
      "    upstream_nodes: \"latest_blessed_model_resolver\"\r\n",
      "    downstream_nodes: \"Pusher\"\r\n",
      "    execution_options {\r\n",
      "      caching_options {\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "nodes {\r\n",
      "  pipeline_node {\r\n",
      "    node_info {\r\n",
      "      type {\r\n",
      "        name: \"tfx.components.infra_validator.component.InfraValidator\"\r\n",
      "      }\r\n",
      "      id: \"InfraValidator\"\r\n",
      "    }\r\n",
      "    contexts {\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"pipeline\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          field_value {\r\n",
      "            string_value: \"tfx-titanic-training\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"pipeline_run\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          runtime_parameter {\r\n",
      "            name: \"pipeline_run_id\"\r\n",
      "            type: STRING\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"node\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          field_value {\r\n",
      "            string_value: \"tfx-titanic-training.InfraValidator\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    inputs {\r\n",
      "      inputs {\r\n",
      "        key: \"examples\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"CsvExampleGen\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.CsvExampleGen\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"Examples\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"examples\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      inputs {\r\n",
      "        key: \"model\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"Trainer\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.Trainer\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"Model\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"model\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    outputs {\r\n",
      "      outputs {\r\n",
      "        key: \"blessing\"\r\n",
      "        value {\r\n",
      "          artifact_spec {\r\n",
      "            type {\r\n",
      "              name: \"InfraBlessing\"\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    parameters {\r\n",
      "      parameters {\r\n",
      "        key: \"request_spec\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"{\\n  \\\"num_examples\\\": 3,\\n  \\\"tensorflow_serving\\\": {}\\n}\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      parameters {\r\n",
      "        key: \"serving_spec\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"{\\n  \\\"local_docker\\\": {},\\n  \\\"tensorflow_serving\\\": {\\n    \\\"tags\\\": [\\n      \\\"latest\\\"\\n    ]\\n  }\\n}\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      parameters {\r\n",
      "        key: \"validation_spec\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"{\\n  \\\"max_loading_time_seconds\\\": 60,\\n  \\\"num_tries\\\": 3\\n}\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    upstream_nodes: \"CsvExampleGen\"\r\n",
      "    upstream_nodes: \"Trainer\"\r\n",
      "    downstream_nodes: \"Pusher\"\r\n",
      "    execution_options {\r\n",
      "      caching_options {\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "nodes {\r\n",
      "  pipeline_node {\r\n",
      "    node_info {\r\n",
      "      type {\r\n",
      "        name: \"tfx.components.pusher.component.Pusher\"\r\n",
      "      }\r\n",
      "      id: \"Pusher\"\r\n",
      "    }\r\n",
      "    contexts {\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"pipeline\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          field_value {\r\n",
      "            string_value: \"tfx-titanic-training\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"pipeline_run\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          runtime_parameter {\r\n",
      "            name: \"pipeline_run_id\"\r\n",
      "            type: STRING\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      contexts {\r\n",
      "        type {\r\n",
      "          name: \"node\"\r\n",
      "        }\r\n",
      "        name {\r\n",
      "          field_value {\r\n",
      "            string_value: \"tfx-titanic-training.Pusher\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    inputs {\r\n",
      "      inputs {\r\n",
      "        key: \"infra_blessing\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"InfraValidator\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.InfraValidator\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"InfraBlessing\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"blessing\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      inputs {\r\n",
      "        key: \"model\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"Trainer\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.Trainer\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"Model\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"model\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      inputs {\r\n",
      "        key: \"model_blessing\"\r\n",
      "        value {\r\n",
      "          channels {\r\n",
      "            producer_node_query {\r\n",
      "              id: \"Evaluator\"\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"pipeline_run\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                runtime_parameter {\r\n",
      "                  name: \"pipeline_run_id\"\r\n",
      "                  type: STRING\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            context_queries {\r\n",
      "              type {\r\n",
      "                name: \"node\"\r\n",
      "              }\r\n",
      "              name {\r\n",
      "                field_value {\r\n",
      "                  string_value: \"tfx-titanic-training.Evaluator\"\r\n",
      "                }\r\n",
      "              }\r\n",
      "            }\r\n",
      "            artifact_query {\r\n",
      "              type {\r\n",
      "                name: \"ModelBlessing\"\r\n",
      "              }\r\n",
      "            }\r\n",
      "            output_key: \"blessing\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    outputs {\r\n",
      "      outputs {\r\n",
      "        key: \"pushed_model\"\r\n",
      "        value {\r\n",
      "          artifact_spec {\r\n",
      "            type {\r\n",
      "              name: \"PushedModel\"\r\n",
      "            }\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    parameters {\r\n",
      "      parameters {\r\n",
      "        key: \"custom_config\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"null\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "      parameters {\r\n",
      "        key: \"push_destination\"\r\n",
      "        value {\r\n",
      "          field_value {\r\n",
      "            string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"/home/michal/serving_model\\\"\\n  }\\n}\"\r\n",
      "          }\r\n",
      "        }\r\n",
      "      }\r\n",
      "    }\r\n",
      "    upstream_nodes: \"Evaluator\"\r\n",
      "    upstream_nodes: \"InfraValidator\"\r\n",
      "    upstream_nodes: \"Trainer\"\r\n",
      "    execution_options {\r\n",
      "      caching_options {\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "runtime_spec {\r\n",
      "  pipeline_root {\r\n",
      "    runtime_parameter {\r\n",
      "      name: \"pipeline_root\"\r\n",
      "      type: STRING\r\n",
      "      default_value {\r\n",
      "        string_value: \"/home/michal/kubeflow-artifact-store/tfx-titanic-training/{{workflow.uid}}\"\r\n",
      "      }\r\n",
      "    }\r\n",
      "  }\r\n",
      "  pipeline_run_id {\r\n",
      "    runtime_parameter {\r\n",
      "      name: \"pipeline_run_id\"\r\n",
      "      type: STRING\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "execution_mode: SYNC\r\n",
      "deployment_config {\r\n",
      "  type_url: \"type.googleapis.com/tfx.orchestration.IntermediateDeploymentConfig\"\r\n",
      "  value: \"\\n\\256\\002\\n\\tEvaluator\\022\\240\\002\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\323\\001\\n,\\n*tfx.components.evaluator.executor.Executor\\022\\025--runner=DirectRunner\\022\\037--experiments=shuffle_mode=auto\\022\\037--project=cloud-training-281409\\0224--temp_location=/home/michal/artifact-store/beam/tmp\\022\\024--region=us-central1\\n\\276\\002\\n\\016InfraValidator\\022\\253\\002\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022\\327\\001\\n0tfx.components.infra_validator.executor.Executor\\022\\025--runner=DirectRunner\\022\\037--experiments=shuffle_mode=auto\\022\\037--project=cloud-training-281409\\0224--temp_location=/home/michal/artifact-store/beam/tmp\\022\\024--region=us-central1\\n\\266\\002\\n\\007Trainer\\022\\252\\002\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022\\326\\001\\n/tfx.components.trainer.executor.GenericExecutor\\022\\025--runner=DirectRunner\\022\\037--experiments=shuffle_mode=auto\\022\\037--project=cloud-training-281409\\0224--temp_location=/home/michal/artifact-store/beam/tmp\\022\\024--region=us-central1\\n\\302\\002\\n\\020ExampleValidator\\022\\255\\002\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022\\331\\001\\n2tfx.components.example_validator.executor.Executor\\022\\025--runner=DirectRunner\\022\\037--experiments=shuffle_mode=auto\\022\\037--project=cloud-training-281409\\0224--temp_location=/home/michal/artifact-store/beam/tmp\\022\\024--region=us-central1\\n\\253\\002\\n\\005Tuner\\022\\241\\002\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022\\315\\001\\n&tfx.components.tuner.executor.Executor\\022\\025--runner=DirectRunner\\022\\037--experiments=shuffle_mode=auto\\022\\037--project=cloud-training-281409\\0224--temp_location=/home/michal/artifact-store/beam/tmp\\022\\024--region=us-central1\\n\\267\\002\\n\\rStatisticsGen\\022\\245\\002\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\330\\001\\n1\\n/tfx.components.statistics_gen.executor.Executor\\022\\025--runner=DirectRunner\\022\\037--experiments=shuffle_mode=auto\\022\\037--project=cloud-training-281409\\0224--temp_location=/home/michal/artifact-store/beam/tmp\\022\\024--region=us-central1\\n\\255\\002\\n\\006Pusher\\022\\242\\002\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022\\316\\001\\n\\'tfx.components.pusher.executor.Executor\\022\\025--runner=DirectRunner\\022\\037--experiments=shuffle_mode=auto\\022\\037--project=cloud-training-281409\\0224--temp_location=/home/michal/artifact-store/beam/tmp\\022\\024--region=us-central1\\n\\304\\002\\n\\rCsvExampleGen\\022\\262\\002\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\345\\001\\n>\\n<tfx.components.example_gen.csv_example_gen.executor.Executor\\022\\025--runner=DirectRunner\\022\\037--experiments=shuffle_mode=auto\\022\\037--project=cloud-training-281409\\0224--temp_location=/home/michal/artifact-store/beam/tmp\\022\\024--region=us-central1\\n\\256\\002\\n\\tTransform\\022\\240\\002\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022\\323\\001\\n,\\n*tfx.components.transform.executor.Executor\\022\\025--runner=DirectRunner\\022\\037--experiments=shuffle_mode=auto\\022\\037--project=cloud-training-281409\\0224--temp_location=/home/michal/artifact-store/beam/tmp\\022\\024--region=us-central1\\n\\264\\002\\n\\tSchemaGen\\022\\246\\002\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022\\322\\001\\n+tfx.components.schema_gen.executor.Executor\\022\\025--runner=DirectRunner\\022\\037--experiments=shuffle_mode=auto\\022\\037--project=cloud-training-281409\\0224--temp_location=/home/michal/artifact-store/beam/tmp\\022\\024--region=us-central1\\022\\230\\001\\n\\rCsvExampleGen\\022\\206\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0223\\n1tfx.components.example_gen.driver.FileBasedDriver\"\r\n",
      "}\r\n",
      "\r\n",
      "INFO:absl:Adding upstream dependencies for component csvexamplegen\r\n",
      "INFO:absl:Adding upstream dependencies for component import-user-schema\r\n",
      "INFO:absl:Adding upstream dependencies for component latest-blessed-model-resolver\r\n",
      "INFO:absl:Adding upstream dependencies for component statisticsgen\r\n",
      "INFO:absl:   ->  Component: csvexamplegen\r\n",
      "INFO:absl:Adding upstream dependencies for component transform\r\n",
      "INFO:absl:   ->  Component: csvexamplegen\r\n",
      "INFO:absl:   ->  Component: import-user-schema\r\n",
      "INFO:absl:Adding upstream dependencies for component examplevalidator\r\n",
      "INFO:absl:   ->  Component: statisticsgen\r\n",
      "INFO:absl:   ->  Component: import-user-schema\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:absl:Adding upstream dependencies for component schemagen\n",
      "INFO:absl:   ->  Component: statisticsgen\n",
      "INFO:absl:Adding upstream dependencies for component tuner\n",
      "INFO:absl:   ->  Component: transform\n",
      "INFO:absl:Adding upstream dependencies for component trainer\n",
      "INFO:absl:   ->  Component: tuner\n",
      "INFO:absl:   ->  Component: import-user-schema\n",
      "INFO:absl:   ->  Component: transform\n",
      "INFO:absl:Adding upstream dependencies for component evaluator\n",
      "INFO:absl:   ->  Component: latest-blessed-model-resolver\n",
      "INFO:absl:   ->  Component: csvexamplegen\n",
      "INFO:absl:   ->  Component: trainer\n",
      "INFO:absl:Adding upstream dependencies for component infravalidator\n",
      "INFO:absl:   ->  Component: csvexamplegen\n",
      "INFO:absl:   ->  Component: trainer\n",
      "INFO:absl:Adding upstream dependencies for component pusher\n",
      "INFO:absl:   ->  Component: infravalidator\n",
      "INFO:absl:   ->  Component: trainer\n",
      "INFO:absl:   ->  Component: evaluator\n",
      "Please access the pipeline detail page at http://localhost:8080/#/pipelines/details/8670e379-208e-4a6c-8c80-3b1d78558680\n",
      "Pipeline \"tfx-titanic-training\" created successfully.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#!tfx pipeline create --pipeline_path=kubeflow_runner.py --endpoint={ENDPOINT} \\\n",
    "#--build_target_image={CUSTOM_TFX_IMAGE}\n",
    "!tfx pipeline create --pipeline_path=kubeflow_runner.py --engine kubeflow --endpoint={ENDPOINT}\n",
    "#--build_target_image={CUSTOM_TFX_IMAGE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint**: review the [TFX CLI documentation](https://www.tensorflow.org/tfx/guide/cli#create) on the \"pipeline group\" to create your pipeline. You will need to specify the `--pipeline_path` to point at the pipeline DSL and runner defined locally in `runner.py`, `--endpoint`, and `--build_target_image` arguments using the environment variables specified above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you should see a `build.yaml` file in your pipeline folder created by skaffold. The TFX CLI compile triggers a custom container to be built with skaffold using the instructions in the `Dockerfile`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to redeploy the pipeline you can first delete the previous version using `tfx pipeline delete` or you can update the pipeline in-place using `tfx pipeline update`.\n",
    "\n",
    "To delete the pipeline:\n",
    "\n",
    "`tfx pipeline delete --pipeline_name {PIPELINE_NAME} --endpoint {ENDPOINT}`\n",
    "\n",
    "To update the pipeline:\n",
    "\n",
    "`tfx pipeline update --pipeline_path runner.py --endpoint {ENDPOINT}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx pipeline delete --pipeline_name {PIPELINE_NAME} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-22 16:32:07.717953: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2021-06-22 16:32:07.718003: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "CLI\n",
      "Updating pipeline\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "INFO:absl:PIPELINE_ROOT=gs://cloud-training-281409-kubeflowpipelines-default/tfx-titanic-training/{{workflow.uid}}\n",
      "INFO:absl:pipeline_name: tfx-titanic-training\n",
      "INFO:absl:pipeline root: gs://cloud-training-281409-kubeflowpipelines-default/tfx-titanic-training/{{workflow.uid}}\n",
      "INFO:absl:data_root_uri for training: gs://cloud-training-281409-kubeflowpipelines-default/tfx-template/data/titanic\n",
      "INFO:absl:train_steps for training: 2000\n",
      "INFO:absl:tuner_steps for tuning: 400\n",
      "INFO:absl:eval_steps for evaluating: 100\n",
      "INFO:absl:os default list dir: ['__init__.py', 'schema', 'local_runner.py', 'kubeflow_runner.py', 'tfx-titanic-training.tar.gz', 'pipelines.py', 'client', 'tests', 'model.py', 'hyperparameters', '__pycache__', 'beam_dag_runner.py', 'features.py', 'lib', 'preprocessing.py', 'airflow_runner.py', 'config.py', 'Dockerfile', 'pipeline_args.py']\n",
      "INFO:absl:schema_proper_folder: /home/michal/PycharmProjects/tfx-titanic-training/pipeline/schema\n",
      "INFO:absl:preprocessing_proper_file: /home/michal/PycharmProjects/tfx-titanic-training/pipeline/preprocessing.py\n",
      "INFO:absl:model_proper_file: /home/michal/PycharmProjects/tfx-titanic-training/pipeline/model.py\n",
      "INFO:absl:hyperparameters_proper_folder: /home/michal/PycharmProjects/tfx-titanic-training/pipeline/hyperparameters\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:train_steps: {\"__class__\": \"RuntimeParameter\", \"__module__\": \"tfx.orchestration.data_types\", \"__tfx_object_type__\": \"jsonable\", \"default\": 2000, \"description\": null, \"name\": \"train-steps\", \"ptype\": {\"__class__\": \"int\", \"__module__\": \"builtins\", \"__tfx_object_type__\": \"class\"}}\n",
      "INFO:absl:eval_steps: {\"__class__\": \"RuntimeParameter\", \"__module__\": \"tfx.orchestration.data_types\", \"__tfx_object_type__\": \"jsonable\", \"default\": 100, \"description\": null, \"name\": \"eval-steps\", \"ptype\": {\"__class__\": \"int\", \"__module__\": \"builtins\", \"__tfx_object_type__\": \"class\"}}\n",
      "INFO:absl:tuner_args: {'module_file': '/home/michal/PycharmProjects/tfx-titanic-training/pipeline/model.py', 'examples': Channel(\n",
      "    type_name: Examples\n",
      "    artifacts: []\n",
      "    additional_properties: {}\n",
      "    additional_custom_properties: {}\n",
      "), 'transform_graph': Channel(\n",
      "    type_name: TransformGraph\n",
      "    artifacts: []\n",
      "    additional_properties: {}\n",
      "    additional_custom_properties: {}\n",
      "), 'train_args': {'num_steps': 400}, 'eval_args': {'num_steps': 20}, 'custom_config': {'max_trials': 50, 'is_local_run': 0}}\n",
      "WARNING:absl:`custom_executor_spec` is going to be deprecated.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/bin/tfx\", line 8, in <module>\n",
      "    sys.exit(cli_group())\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 829, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 782, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 1259, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 1259, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 1066, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/decorators.py\", line 73, in new_func\n",
      "    return ctx.invoke(f, obj, *args, **kwargs)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/click/core.py\", line 610, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/tfx/tools/cli/commands/pipeline.py\", line 215, in update_pipeline\n",
      "    handler_factory.create_handler(ctx.flags_dict).update_pipeline()\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/tfx/tools/cli/handler/kubeflow_handler.py\", line 106, in update_pipeline\n",
      "    self.create_pipeline(update=True)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/tfx/tools/cli/handler/kubeflow_handler.py\", line 90, in create_pipeline\n",
      "    context = self.execute_dsl(patcher)\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/tfx/tools/cli/handler/base_handler.py\", line 173, in execute_dsl\n",
      "    loader.exec_module(\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"kubeflow_runner.py\", line 168, in <module>\n",
      "    kubeflowRunner.run()\n",
      "  File \"kubeflow_runner.py\", line 143, in run\n",
      "    pipeline.create_pipeline(\n",
      "  File \"/home/michal/PycharmProjects/tfx-titanic-training/pipeline/pipelines.py\", line 356, in create_pipeline\n",
      "    pusher = Pusher(**pusher_args)  # pylint: disable=unused-variable\n",
      "  File \"/home/michal/venv/ML-tfx-0.30.0/lib/python3.8/site-packages/tfx/components/pusher/component.py\", line 105, in __init__\n",
      "    raise ValueError('push_destination is required unless a '\n",
      "ValueError: push_destination is required unless a custom_executor_spec is supplied that does not require it.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx pipeline update --pipeline_path kubeflow_runner.py --engine kubeflow --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and monitor a pipeline run with the TFX CLI\n",
    "\n",
    "After the pipeline has been deployed, you can trigger and monitor pipeline runs using TFX CLI.\n",
    "\n",
    "*Hint*: review the [TFX CLI documentation](https://www.tensorflow.org/tfx/guide/cli#run_group) on the \"run group\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-18 13:24:30.832593: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2021-06-18 13:24:30.832651: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "CLI\n",
      "Creating a run for pipeline: tfx-titanic-training\n",
      "Run created for pipeline: tfx-titanic-training\n",
      "+======================+======================================+========+===========================+===========================================================================+\n",
      "| pipeline_name        | run_id                               | status | created_at                | link                                                                      |\n",
      "+======================+======================================+========+===========================+===========================================================================+\n",
      "| tfx-titanic-training | 16bfe607-d885-4ca5-851b-266e4cf73e9e | None   | 2021-06-18T11:24:35+00:00 | http://localhost:8080/#/runs/details/16bfe607-d885-4ca5-851b-266e4cf73e9e |\n",
      "+======================+======================================+========+===========================+===========================================================================+\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx run create --pipeline_name={PIPELINE_NAME} --engine kubeflow --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the status of existing pipeline runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-18 13:26:47.295516: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/cuda/include:/usr/lib/cuda/lib64:\n",
      "2021-06-18 13:26:47.295579: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "CLI\n",
      "Listing all runs of pipeline: tfx-titanic-training\n",
      "+======================+======================================+========+===========================+===========================================================================+\n",
      "| pipeline_name        | run_id                               | status | created_at                | link                                                                      |\n",
      "+======================+======================================+========+===========================+===========================================================================+\n",
      "| tfx-titanic-training | fd11ec62-e534-4dc6-bb52-2afcf655949f | Failed | 2021-06-16T20:55:27+00:00 | http://localhost:8080/#/runs/details/fd11ec62-e534-4dc6-bb52-2afcf655949f |\n",
      "+----------------------+--------------------------------------+--------+---------------------------+---------------------------------------------------------------------------+\n",
      "| tfx-titanic-training | 6a846f91-adde-493c-867d-25b204fb7248 | Failed | 2021-06-17T14:53:23+00:00 | http://localhost:8080/#/runs/details/6a846f91-adde-493c-867d-25b204fb7248 |\n",
      "+----------------------+--------------------------------------+--------+---------------------------+---------------------------------------------------------------------------+\n",
      "| tfx-titanic-training | 16bfe607-d885-4ca5-851b-266e4cf73e9e | Failed | 2021-06-18T11:24:35+00:00 | http://localhost:8080/#/runs/details/16bfe607-d885-4ca5-851b-266e4cf73e9e |\n",
      "+======================+======================================+========+===========================+===========================================================================+\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx run list --pipeline_name {PIPELINE_NAME} --engine kubeflow --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the status of a given run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID='[YOUR RUN ID]'\n",
    "\n",
    "!tfx run status --pipeline_name {PIPELINE_NAME} --run_id {RUN_ID} --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile your pipeline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLI\n",
      "Compiling pipeline\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "WARNING:absl:`instance_name` is deprecated, please set node id directly using`with_id()` or `.id` setter.\n",
      "\u001b[0mPipeline compiled successfully.\n",
      "Pipeline package path: /home/jupyter/ml-gcp-pipeline/tfx_titanic_pipeline/pipeline/tfx-titanic-training.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!tfx pipeline compile --engine kubeflow --pipeline_path runner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy your pipeline container to AI Platform Pipelines with the TFX CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tfx pipeline update --pipeline_path kubeflow_runner.py --endpoint {ENDPOINT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger a pipeline run from the Kubeflow Pipelines UI\n",
    "\n",
    "On the [AI Platform Pipelines](https://console.cloud.google.com/ai-platform/pipelines/clusters) page, click `OPEN PIPELINES DASHBOARD`. A new browser tab will open. Select the `Pipelines` tab to the left where you see the `PIPELINE_NAME` pipeline you deployed previously. You should see 2 pipeline versions. \n",
    "\n",
    "Click on the most recent pipeline version with tuning enabled which will open up a window with a graphical display of your TFX pipeline directed graph. \n",
    "\n",
    "Next, click the `Create a run` button. Verify the `Pipeline name` and `Pipeline version` are pre-populated and optionally provide a `Run name` and `Experiment` to logically group the run metadata under before hitting `Start` to trigger the pipeline run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "A full pipeline run with tuning enabled will take about 50 minutes and can be executed in parallel while the previous pipeline run without tuning continues running. \n",
    "\n",
    "Take the time to review the pipeline metadata artifacts created in the GCS artifact repository for each component including data splits, your Tensorflow SavedModel, model evaluation results, etc. as the pipeline executes. In the GCP console, you can also view the Dataflow jobs for pipeline data processing as well as the AI Platform Training jobs for model training and tuning.\n",
    "\n",
    "When your pipelines runs are complete, review your model versions on Cloud AI Platform Prediction and model evaluation metrics. Did your model performance improve with hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you learned how to build and deploy a TFX pipeline with the TFX CLI and then update, build and deploy a new pipeline with automatic hyperparameter tuning. You practiced triggered continuous pipeline runs using the TFX CLI as well as the Kubeflow Pipelines UI.\n",
    "\n",
    "\n",
    "In the next lab, you will construct a Cloud Build CI/CD workflow that further automates the building and deployment of the TensorFlow WideDeep Classifer pipeline code introduced in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=-1>Licensed under the Apache License, Version 2.0 (the \\\"License\\\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at [https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \\\"AS IS\\\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the License for the specific language governing permissions and limitations under the License.</font>"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
